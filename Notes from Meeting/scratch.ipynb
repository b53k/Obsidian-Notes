{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Gaussian Processes (from Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, xp):\n",
    "    '''k(x,x') = sigma^2 exp(-0.5*length^2*|x-x'|^2)'''\n",
    "    σ = 1\n",
    "    length = 1\n",
    "    sq_norm = scipy.spatial.distance.cdist(x, xp, 'sqeuclidean')\n",
    "    return σ**2 * np.exp(-0.5*sq_norm*length**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from Gaussian Process Distribution\n",
    "pts = 100 # number of points in each function\n",
    "n = 5 # number of functions to sample\n",
    "\n",
    "# Independent Variable Samples\n",
    "X = np.linspace(0,5, pts)\n",
    "X = X.reshape(-1,1)\n",
    "Σ = kernel(X,X)\n",
    "fx = np.random.multivariate_normal(mean = np.zeros(pts), cov = Σ, size = n)\n",
    "\n",
    "plt.title('RBF Kernel: $k(x,x\\')$')\n",
    "plt.imshow(Σ, cmap = 'viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('X')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "for i in range(n):\n",
    "    plt.plot(X, fx[i])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(0,5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y = f(X)')\n",
    "plt.title('Priors sampled from Gaussian Process with RBF Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction from Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(X1, y1, X2, kernel, noise = None, samples = None):\n",
    "    '''\n",
    "    Compute posterior mean and covariance i.e. mu_(2|1) and cov_(2|1)\n",
    "    y1 = f(x1)\n",
    "    '''\n",
    "    Σ11 = kernel(X1, X1)\n",
    "\n",
    "    if noise is not None:\n",
    "        err = (noise**2) * np.eye(Σ11.shape[0])\n",
    "        Σ11 += err\n",
    "        \n",
    "    Σ22 = kernel(X2, X2)\n",
    "    Σ12 = kernel(X1, X2)\n",
    "\n",
    "    sol = scipy.linalg.solve(Σ11, Σ12, assume_a = 'pos').T\n",
    "    #μ1 = np.mean(y1)\n",
    "    μ1 = 0 # assume prior mean is 0\n",
    "    μ2 = np.mean(X2)\n",
    "\n",
    "    μ = μ2 + sol @ (y1 - μ1)\n",
    "    Σ = Σ22 - (sol @ Σ12)\n",
    "\n",
    "    return μ, Σ   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the true function\n",
    "f_sin = lambda x: (np.sin(x)).flatten()\n",
    "n1 = 10 # number of points to condition on (training points)\n",
    "n2 = 70 # number of points in posterior (test points)\n",
    "ny = 5 # number of functions that will be sampled from posterior\n",
    "\n",
    "# Sample observations\n",
    "X1 = np.random.uniform(-4, 4, size = (n1, 1))\n",
    "y1 = f_sin(X1)\n",
    "\n",
    "# Predict points at uniform spacing to capture funciton\n",
    "X2 = np.linspace(-6, 6, n2).reshape(-1,1)\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "μ2_1, Σ2_1 = posterior(X1, y1, X2, kernel = kernel, noise = 0.2, samples = n1)\n",
    "\n",
    "# Compute standard deviation at test points to be plotted\n",
    "σ2 = np.sqrt(np.diag(Σ2_1))\n",
    "\n",
    "# Draw some samples from the posterior\n",
    "y2 = np.random.multivariate_normal(mean = μ2_1, cov = Σ2_1, size = ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(X2, f_sin(X2), 'b--',label = '$sin(x)$')\n",
    "plt.scatter(X1, y1, color = 'red',label = '($x_1, y_1$)')\n",
    "plt.plot(X2, μ2_1, color = 'red', label = '$\\mu_{2|1}$')\n",
    "plt.fill_between(X2.flatten(), μ2_1 - σ2, μ2_1 + σ2, color = 'blue', alpha = 0.1, label = '$\\pm \\sigma$')\n",
    "plt.plot()\n",
    "plt.legend()\n",
    "plt.xlim(-6,6)\n",
    "plt.title('Posterior Distribution')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Sampling from Posterior $\\mathbb{P}(x_2|x_1)$')\n",
    "plt.plot(X2, y2.T)\n",
    "plt.xlim(-6,6)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, xp, σ, l):\n",
    "    '''k(x,x') = sigma^2 exp(-0.5*length^2*|x-x'|^2)'''\n",
    "    length = l\n",
    "    sq_norm = (scipy.spatial.distance.cdist(x, xp))**2\n",
    "    return σ**2 * np.exp(-0.5*sq_norm/(length**2))\n",
    "\n",
    "def dKdL(x1, x2, σ, l):\n",
    "    '''\n",
    "    computes partial derivative of K w.r.t length (l)\n",
    "    arg: x1 = (N1, D), x2 = (N2, D)\n",
    "    return: (N1, N2)\n",
    "    '''\n",
    "    sq_norm = (scipy.spatial.distance.cdist(x1, x2))**2\n",
    "    return (σ**2) * np.exp(-sq_norm/(2*l**2)) * (sq_norm) / (l**3)\n",
    "\n",
    "def dKdσ(x1, x2, σ, l):\n",
    "    '''\n",
    "    computes partal derivatice of K w.r.t sigma (std not variance)\n",
    "    arg: x1 = (N1, D), x2 = (N2, D)\n",
    "    return: (N1, N2)\n",
    "    '''\n",
    "    sq_norm = (scipy.spatial.distance.cdist(x1, x2))**2\n",
    "    return 2*σ*np.exp(-sq_norm/(2*l**2))\n",
    "\n",
    "def dLdt(a, iKxx, dKdt):\n",
    "    '''\n",
    "    computes gradient of log marginal likelihood w.r.t. a hyper-parameter\n",
    "    i.e. either sigma or length\n",
    "    '''\n",
    "    return 0.5**np.trace(np.dot(a @ a.T - iKxx, dKdt))\n",
    "    \n",
    "def f_opt(kernel, X, y, σ, l):\n",
    "    '''\n",
    "    Evalaute Negative-Log Marginal Likelihood\n",
    "    '''\n",
    "    σ_n = 0.1 # std of noise hard-coded for now\n",
    "    K = kernel(X,X, σ = σ, l = l) + (σ_n**2)*np.eye(X.shape[0])\n",
    "    L = np.linalg.cholesky(K) + 1e-12                   # Cholesky decomposition\n",
    "    a = np.linalg.solve(L.T, np.linalg.solve(L, y))     # compute alpha\n",
    "\n",
    "    #log_likelihood = -0.5 * y.T @ a - 0.5 * np.trace(np.log(L)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "    log_likelihood = -0.5 * y.T @ a - 0.5 * np.log(np.linalg.det(K)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "\n",
    "    return -log_likelihood\n",
    "\n",
    "def grad_f(params, X, y):\n",
    "    '''\n",
    "    Compute gradient of objective function w.r.t. two parameters\n",
    "    '''\n",
    "    l, σ = params\n",
    "    σ_n = 0.0 # std of noise hard-coded for now\n",
    "    K = kernel(X,X, σ = σ, l = l) + (σ_n**2)*np.eye(X.shape[0])\n",
    "    L = np.linalg.cholesky(K) + 1e-12# Cholesky decomposition\n",
    "    a = np.linalg.solve(L.T, np.linalg.solve(L, y))  # compute alpha\n",
    "\n",
    "    inv_k = np.linalg.inv(K)\n",
    "\n",
    "    print ('dKdsigma:', dKdσ(X, X, σ, l))\n",
    "    print ('dKdlength:', dKdL(X, X, σ, l))\n",
    "\n",
    "    grad = np.empty([2,])\n",
    "    grad[0] = dLdt(a = a, iKxx = inv_k, dKdt = dKdσ(X, X, σ, l)) # gradient w.r.t sigma\n",
    "    grad[1] = dLdt(a = a, iKxx = inv_k, dKdt = dKdL(X, X, σ, l)) # gradient w.r.t length\n",
    "    return grad\n",
    "\n",
    "def marginal(params, X, y):\n",
    "    '''\n",
    "    Evalaute Negative-Log Marginal Likelihood\n",
    "    '''\n",
    "    #print (params)\n",
    "    l, σ = params\n",
    "    σ_n = 0.1 # std of noise hard-coded for now\n",
    "    K = kernel(X, X, σ = σ, l = l) + (σ_n**2)*np.eye(X.shape[0])\n",
    "    L = np.linalg.cholesky(K) + 1e-12                   # Cholesky decomposition\n",
    "    a = np.linalg.solve(L.T, np.linalg.solve(L, y))     # compute alpha\n",
    "\n",
    "    #log_likelihood = -0.5 * y.T @ a - 0.5 * np.trace(np.log(L)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "    log_likelihood = -0.5 * y.T @ a - 0.5 * np.log(np.linalg.det(K)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "\n",
    "    return -log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sin = lambda x: (np.sin(x)).flatten()\n",
    "X = np.random.uniform(-4, 4, size = (10, 1))\n",
    "y = f_sin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = [10**-3, 10**3]\n",
    "bound = [lim, lim]\n",
    "start = [0.01, 0.05]   # initial hyper-parameter\n",
    "result = scipy.optimize.minimize(fun = marginal, x0 = start, args = (X, y), jac = grad_f, method =  'SLSQP', options = {'disp':True}, bounds = bound, tol = 0.0001)\n",
    "result1 = scipy.optimize.minimize(fun = marginal, x0 = start, args = (X, y), method =  'SLSQP', options = {'disp':True}, bounds = bound, tol = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (result)\n",
    "print (result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.linspace(10**-3, 10**2, 1000)\n",
    "S = np.linspace(10**-3, 10**3, 1000)\n",
    "σ, l = np.meshgrid(L, S)\n",
    "\n",
    "func_val = np.zeros_like(σ)\n",
    "\n",
    "for i in range(σ.shape[0]):\n",
    "    for j in range(l.shape[0]):\n",
    "        func_val[i, j] = f_opt(kernel = kernel, X = X.reshape(-1,1), y = y.reshape(-1,1), σ = S[i], l = L[j])\n",
    "\n",
    "plt.contourf(σ, l, func_val, cmap = 'plasma')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.scatter(result.x[0], result.x[1], color = 'black', marker = 'x', label = 'with Jac')\n",
    "plt.scatter(result1.x[0], result1.x[1], color = 'white', marker = 'x', label = 'without Jac')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Characteristic Length (l)')\n",
    "plt.ylabel('Sigma $\\sigma$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def metropolis_hastings(target_density, proposal_density, num_samples, initial_state, burn_in=1000):\n",
    "    \"\"\"\n",
    "    Metropolis-Hastings algorithm for sampling from a target density using a proposal density.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_density : function\n",
    "        The target probability density function.\n",
    "    proposal_density : function\n",
    "        The proposal probability density function.\n",
    "    num_samples : int\n",
    "        The number of samples to generate.\n",
    "    initial_state : float or array_like\n",
    "        The initial state of the Markov chain.\n",
    "    burn_in : int, optional\n",
    "        The number of samples to discard at the beginning of the chain as burn-in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    samples : array_like\n",
    "        The generated samples.\n",
    "    acceptance_rate : float\n",
    "        The acceptance rate of the Metropolis-Hastings algorithm.\n",
    "    \"\"\"\n",
    "    samples = [initial_state]\n",
    "    all_samples = [initial_state]\n",
    "    num_accepted = 0\n",
    "\n",
    "    current_state = initial_state\n",
    "    for i in range(num_samples + burn_in):\n",
    "\n",
    "        proposed_state = proposal_density(current_state)\n",
    "        hasting_ratio = proposal_density(current_state) / proposal_density(proposed_state)\n",
    "        metropolis_ratio = target_density(proposed_state) / target_density(current_state)\n",
    "        ratio = metropolis_ratio*hasting_ratio\n",
    "        acceptance_prob = min(ratio, 1)\n",
    "\n",
    "        if np.random.rand() < acceptance_prob:\n",
    "            current_state = proposed_state\n",
    "            num_accepted += 1\n",
    "\n",
    "        if i >= burn_in:\n",
    "            samples.append(current_state)\n",
    "    \n",
    "        all_samples.append(current_state)\n",
    "\n",
    "    acceptance_rate = num_accepted / num_samples\n",
    "\n",
    "    return samples, acceptance_rate, all_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x):\n",
    "    return 0.5*(1 / np.sqrt(2 * np.pi * 1**2) * np.exp(-(x + 2)**2 / (2 * 1**2))) + 0.5 *(1 / np.sqrt(2 * np.pi * 1**2) * np.exp(-(x - 2)**2 / (2 * 1**2)))\n",
    "    #return np.exp(-0.5 * ((x - 3) / 0.8)**2) + np.exp(-0.5 * ((x + 1) / 1.2)**2)\n",
    "    \n",
    "\n",
    "def proposal(x):\n",
    "    return np.random.normal(x, 1)\n",
    "\n",
    "\n",
    "target = lambda x: gaussian(x)\n",
    "proposal = proposal\n",
    "\n",
    "samples, acceptance_rate, all_samples = metropolis_hastings(target, proposal, 10000, initial_state=0)\n",
    "\n",
    "print(f\"Acceptance rate: {acceptance_rate:.3f}\")\n",
    "print(f\"Mean: {np.mean(samples):.3f}\")\n",
    "print(f\"Standard deviation: {np.std(samples):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate samples\n",
    "samples, acceptance_rate, all_samples = metropolis_hastings(target, proposal, 10000, initial_state=0)\n",
    "\n",
    "# Plot true target density\n",
    "x = np.linspace(-5, 6, 100)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x, target(x), color='red', linewidth=2)\n",
    "\n",
    "# Plot histogram of samples\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.5)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Metropolis-Hastings Sampling')\n",
    "plt.legend(['Target Density', 'Generated Samples'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X1, X2, theta):\n",
    "    \"\"\"\n",
    "    Computes the RBF kernel matrix between two sets of input vectors X1 and X2.\n",
    "    \n",
    "    Args:\n",
    "        X1: A numpy array of shape (n1, d), representing the first set of input vectors.\n",
    "        X2: A numpy array of shape (n2, d), representing the second set of input vectors.\n",
    "        lengthscale: The lengthscale parameter of the RBF kernel.\n",
    "        variance: The variance parameter of the RBF kernel.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy array of shape (n1, n2), representing the RBF kernel matrix.\n",
    "\n",
    "    Formula:\n",
    "        dist_sq = ||X1 - X2.T||^2\n",
    "                = ||X1||^2 + ||X2||^2 - 2*X1*X2.T\n",
    "    \"\"\"\n",
    "    variance, lengthscale = theta\n",
    "    dist_sq = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2*np.dot(X1, X2.T)\n",
    "    K = variance * np.exp(-0.5 * dist_sq / (lengthscale ** 2))\n",
    "    return K\n",
    "    \n",
    "def likelihood(X, y, theta, σ_n):\n",
    "    \"\"\"\n",
    "    Computes the likelihood function p(y | X, theta) for a Gaussian Process model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    theta : ndarray\n",
    "        Array of hyperparameters for the kernel function.\n",
    "    X : ndarray\n",
    "        Array of input locations.\n",
    "    y : ndarray\n",
    "        Array of observed data.\n",
    "    sigma_n : float\n",
    "        Observation noise standard deviation.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        Likelihood value.\n",
    "    \"\"\"\n",
    "    K = kernel(X, X, theta = theta) + (σ_n**2)*np.eye(X.shape[0])\n",
    "    L = np.linalg.cholesky(K) + 1e-12                   # Cholesky decomposition\n",
    "    a = np.linalg.solve(L.T, np.linalg.solve(L, y))     # compute alpha\n",
    "\n",
    "    #log_likelihood = -0.5 * y.T @ a - 0.5 * np.trace(np.log(L)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "    log_likelihood = -0.5 * y.T @ a - 0.5 * np.log(np.linalg.det(K)) - 0.5 * X.shape[0] * np.log(2*np.pi)\n",
    "\n",
    "    return np.exp(log_likelihood)\n",
    "\n",
    "def prior(theta, X):\n",
    "    \"\"\"\n",
    "    Computes the prior distribution p(theta) for a set of hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    theta : ndarray\n",
    "        Array of hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        Prior probability density value.\n",
    "    \"\"\"\n",
    "    prior_mean = np.zeros(len(theta))\n",
    "    Σ = kernel(X,X)\n",
    "    return np.random.multivariate_normal(mean = prior_mean, cov = Σ, size = 1)[0][0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
